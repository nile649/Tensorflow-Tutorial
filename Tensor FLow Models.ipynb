{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.constant('Hellow Tensorflow!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    d = sess.run(s).decode('utf-8')\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression Example\n",
    "\n",
    "### A linear regression learning algorithm example using TensorFlow library.\n",
    "\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1],np.float64)\n",
    "train_Y = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3],np.float64)\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# HyperParameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64)\n",
    "Y = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.randn(),name=\"weight\",dtype=tf.float64)\n",
    "b = tf.Variable(np.random.randn(),name=\"bias\",dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print( \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print( \"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print( \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "    writer = tf.summary.FileWriter(\"./Downloads/XOR_logs\", sess.graph)\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACESSING tensorboard\n",
    "# tensorboard --logdir=./Downloads/XOR_logs\n",
    "# in cmd, no space or whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tensorboard --logdir=./Downloads/XOR_logs\n",
    "# tensorboard --logdir=./Downloads/XOR_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf.data for feeding input instead of tf.placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        \n",
    "        print(f\"{value} \", end=\" \")    # 1 2 3 ... 10\t   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The datatype and the shape of the dataset can be retrieved by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.output_types)   # <dtype: 'int64'>\n",
    "print(dataset.output_shapes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterator can run out of values, for example in the following problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))  # \"0\"\n",
    "    print(sess.run(result))  # \"2\"\n",
    "    print(sess.run(result))  # \"4\"\n",
    "    try:\n",
    "      sess.run(result)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # \"End of dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.repeat()\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))  # \"0\"\n",
    "    print(sess.run(result))  # \"2\"\n",
    "    print(sess.run(result))  # \"4\"\n",
    "    print(sess.run(result))  # \"0\"\n",
    "    print(sess.run(result))  # \"2\"\n",
    "    print(sess.run(result))  # \"\n",
    "    try:\n",
    "      sess.run(result)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # \"End of dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializable iterator\n",
    "\n",
    "In the example below, we allow the max range of the iterator to be supplied at runtime using a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7  8  9  "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)    # Take a placeholder to create a dataset\n",
    "iterator = dataset.make_initializable_iterator()      # Create an initializable iterator\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize an iterator over a dataset with 10 elements using placeholder.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 17}) \n",
    "\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        print(f\"{value} \", end=\" \")    # 0 1 2 3 ... 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37507391  0.96357214  0.06132114  0.46031618  0.48979199  0.91276419\n",
      "  0.03700745  0.10200524  0.84653997  0.8921448 ] \n",
      "[ 0.42559302  0.90666747  0.79240739  0.45191526  0.23891401  0.62452793\n",
      "  0.0627588   0.98949814  0.68788385  0.16188097] \n",
      "[ 0.62061393  0.52857888  0.54093575  0.44190741  0.36722231  0.46172905\n",
      "  0.48851907  0.05526829  0.64666057  0.92709422] \n",
      "[ 0.45238733  0.08306217  0.44114304  0.3573519   0.21570289  0.9327848\n",
      "  0.01896024  0.37697995  0.59869933  0.42223799] \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()      # Create an initializable iterator\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "\n",
    "    for i in range(4):\n",
    "      value = sess.run(next_element)\n",
    "      print(f\"{value} \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, Y_batch = mnist.train.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='image') \n",
    "Y = tf.placeholder(tf.int32, [batch_size, 10], name='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable(name=\"weights2\",shape=(784,10),initializer=tf.random_normal_initializer)\n",
    "b = tf.get_variable(name=\"bias2\",shape=(1,10),initializer=tf.zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = tf.matmul(X, w) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=new_y, labels=Y, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = tf.reduce_mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(new_y)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: [ 1.77193427  1.47757149  1.68007851  1.58548582  1.46566081  1.31446922\n",
      "  1.70178783  1.56533277  1.60302567  1.39866567  1.840801    1.3333298\n",
      "  1.61258662  1.43328702  1.50948465  1.66551495  1.50787818  1.61883223\n",
      "  1.39118457  1.51601958  1.72179794  1.54756021  1.53571117  1.40287697\n",
      "  1.80313754  1.65141404  1.36381888  1.56879961  1.89432287  1.53933799\n",
      "  1.24671865  1.47231793  1.26892662  1.35959458  1.49975741  1.41928482\n",
      "  1.66145885  1.48878264  1.5463289   1.79527605  1.38758576  1.71962202\n",
      "  1.58986902  1.33651531  1.36090231  1.47463608  1.5276556   1.68624687\n",
      "  1.59219897  1.46391332  1.4732151   1.78441668  1.56521559  1.5333879\n",
      "  1.64891744  1.47953868  1.67698658  1.65590882  1.34711647  1.52393901\n",
      "  1.47370338  1.44870377  1.4986254   1.37442029  1.56745327  1.58165324\n",
      "  1.64099622  1.45798051  1.44461918  1.76138961  1.73253012  1.61178231\n",
      "  1.24882996  1.51615512  1.36333191  2.01772165  1.51083553  1.32153916\n",
      "  1.60875344  1.52147567  1.37325287  1.63219082  1.29403198  1.66661155\n",
      "  1.23409474  1.47880197  1.46605194  1.27822006  1.32624257  1.50955296\n",
      "  1.73900366  1.54298556  1.30648017  1.4083271   1.80428088  1.51601267\n",
      "  1.53126621  1.45592523  1.4441452   1.43408883  1.616817    1.6864866\n",
      "  1.3758738   1.49391842  1.5024842   1.5780766   1.58086407  1.5917592\n",
      "  1.82112741  1.55116367  1.58330047  1.57454515  1.47067416  1.41034186\n",
      "  1.49253774  1.3559705   1.50778103  1.52067327  1.57202291  1.47873676\n",
      "  1.51637447  1.32794654  1.46021473  1.34901476  1.65533876  1.52542984\n",
      "  1.60197377  1.32159257]\n",
      "Average loss epoch 1: [ 0.40899205  0.37407634  0.44092917  0.58189088  0.51992446  0.48277664\n",
      "  0.49303421  0.52677894  0.51888824  0.51185149  0.59700572  0.39112163\n",
      "  0.61260808  0.4537693   0.48555711  0.54705673  0.50285536  0.47729573\n",
      "  0.66734177  0.50154597  0.60668552  0.53503031  0.60716695  0.53522325\n",
      "  0.54452479  0.56727391  0.60795027  0.54458529  0.44201195  0.52547103\n",
      "  0.43909773  0.44338843  0.67579383  0.55119783  0.37426928  0.45866343\n",
      "  0.55029619  0.49426571  0.39477307  0.47363392  0.50786257  0.56187272\n",
      "  0.4365789   0.65561825  0.61149645  0.41825736  0.50826091  0.64909834\n",
      "  0.66341943  0.5322426   0.50470269  0.53184491  0.56832343  0.50379223\n",
      "  0.43281937  0.40136263  0.48134515  0.52525991  0.58481258  0.43337712\n",
      "  0.41295317  0.62712425  0.42762652  0.6097362   0.63350087  0.43107912\n",
      "  0.49067435  0.54661387  0.57941139  0.6098851   0.65892595  0.57912636\n",
      "  0.4254989   0.45088643  0.4968403   0.3795737   0.55144173  0.57948017\n",
      "  0.56867218  0.56494343  0.57859248  0.60339957  0.56075943  0.50945073\n",
      "  0.53216368  0.72851878  0.42542982  0.47656134  0.62081456  0.46176532\n",
      "  0.75999719  0.48111492  0.50626171  0.57135993  0.48134494  0.49948227\n",
      "  0.63106567  0.46778885  0.4851892   0.50349879  0.69856197  0.6481691\n",
      "  0.7581659   0.51944864  0.49655229  0.57955867  0.72710818  0.42928073\n",
      "  0.58460057  0.37905568  0.51958174  0.67831606  0.45790613  0.52764881\n",
      "  0.57864058  0.53195387  0.61240077  0.51498812  0.62951815  0.62361914\n",
      "  0.60654104  0.60004294  0.66602027  0.52251339  0.50969535  0.76471126\n",
      "  0.48095673  0.63343006]\n",
      "Average loss epoch 2: [ 0.5246855   0.39333722  0.34657225  0.51675177  0.47255477  0.47530773\n",
      "  0.39922041  0.22910303  0.38684583  0.39165744  0.62904978  0.32748482\n",
      "  0.49375394  0.35965836  0.35310897  0.57587743  0.35492641  0.40750575\n",
      "  0.39353344  0.5076133   0.4486773   0.45013624  0.48422259  0.4612475\n",
      "  0.37563312  0.40612319  0.44720256  0.36502707  0.48995233  0.3297444\n",
      "  0.50046033  0.35770845  0.38612163  0.50341231  0.45660323  0.52424306\n",
      "  0.48141623  0.46198848  0.55739039  0.31820643  0.45027235  0.49705288\n",
      "  0.39879531  0.48820934  0.40785658  0.41425797  0.28647667  0.4202781\n",
      "  0.47849393  0.51281357  0.31409058  0.4178302   0.37648261  0.52128023\n",
      "  0.54779899  0.59693861  0.3998476   0.41950148  0.52160603  0.53636748\n",
      "  0.44514063  0.38182878  0.44335154  0.39501607  0.56163114  0.5045082\n",
      "  0.44607851  0.38643128  0.46697816  0.40048459  0.37956217  0.43714637\n",
      "  0.41795158  0.4729875   0.42350638  0.54756564  0.41007736  0.44794238\n",
      "  0.4049997   0.46219388  0.50265497  0.30849054  0.57106048  0.29754469\n",
      "  0.40531555  0.39714247  0.47707224  0.55926496  0.39438051  0.47230861\n",
      "  0.39594129  0.45484847  0.33232257  0.40262783  0.46685961  0.59800601\n",
      "  0.41678125  0.34940553  0.38554895  0.42110801  0.45805874  0.4265334\n",
      "  0.40394577  0.40446743  0.47506508  0.43485838  0.37042603  0.49501845\n",
      "  0.45235881  0.32438624  0.52703792  0.38933015  0.40293255  0.52767867\n",
      "  0.36525282  0.36322671  0.40680102  0.43163836  0.38423428  0.37546381\n",
      "  0.42295468  0.44094738  0.34819925  0.49976844  0.36093488  0.57271349\n",
      "  0.44982374  0.4213604 ]\n",
      "Average loss epoch 3: [ 0.38394198  0.40250641  0.47435728  0.35950363  0.55434644  0.36310303\n",
      "  0.3740049   0.37029362  0.40132973  0.3258642   0.33520433  0.47577983\n",
      "  0.37345478  0.26933044  0.35014659  0.54886073  0.45595708  0.45101765\n",
      "  0.32772225  0.49837297  0.33842835  0.40143827  0.39161333  0.44173428\n",
      "  0.41515991  0.58398986  0.37624907  0.39945891  0.55613112  0.36839601\n",
      "  0.28740007  0.33259338  0.36162755  0.4824371   0.30393216  0.39840552\n",
      "  0.47039431  0.2854045   0.41902444  0.34296268  0.3949143   0.51812553\n",
      "  0.34285173  0.37439847  0.26582897  0.2819716   0.34824756  0.33446458\n",
      "  0.36304623  0.41227421  0.48942268  0.29146084  0.38284975  0.34500527\n",
      "  0.45882869  0.34856677  0.38538373  0.36157432  0.34244254  0.38126546\n",
      "  0.42363319  0.50200599  0.40766129  0.41125187  0.35692245  0.38692489\n",
      "  0.21263428  0.37771404  0.31305355  0.50762576  0.44044179  0.29512838\n",
      "  0.38126236  0.4639813   0.46806839  0.3456049   0.37438598  0.34176999\n",
      "  0.3264339   0.4098559   0.42237133  0.41394085  0.32642072  0.34401509\n",
      "  0.33031142  0.38349631  0.48589855  0.35131481  0.3449052   0.32953224\n",
      "  0.30673027  0.51079756  0.42109007  0.22344439  0.36932424  0.3272672\n",
      "  0.33333933  0.39197588  0.3513923   0.32919368  0.3182084   0.32115668\n",
      "  0.28064051  0.21443379  0.49242628  0.38924715  0.45615742  0.34440234\n",
      "  0.49175224  0.42069572  0.4112269   0.45892996  0.3942647   0.39611334\n",
      "  0.45743072  0.54283071  0.34692562  0.4305023   0.35422948  0.44767764\n",
      "  0.36722213  0.38578445  0.24520957  0.33215675  0.44318488  0.33022606\n",
      "  0.39675185  0.3981393 ]\n",
      "Average loss epoch 4: [ 0.26339617  0.38504732  0.3895866   0.37498131  0.31542706  0.29390195\n",
      "  0.21907835  0.41162509  0.31568485  0.36393648  0.34076798  0.35309201\n",
      "  0.34332126  0.34170383  0.376306    0.27666146  0.38440755  0.29043651\n",
      "  0.27968705  0.40770239  0.32030657  0.37648371  0.43188524  0.32115\n",
      "  0.33402917  0.33665651  0.20159605  0.30419275  0.33997023  0.44948786\n",
      "  0.32863238  0.34168193  0.34275523  0.2237989   0.27442592  0.38390583\n",
      "  0.36578372  0.37130377  0.30829605  0.34819862  0.31292468  0.39478782\n",
      "  0.40692431  0.35773692  0.46820602  0.28472114  0.32363614  0.28874567\n",
      "  0.3632364   0.37304166  0.32889819  0.34041303  0.4428148   0.31036025\n",
      "  0.32544422  0.4558104   0.31183022  0.43554369  0.28486797  0.29261315\n",
      "  0.39655939  0.36519739  0.35570633  0.4377405   0.34869388  0.33139288\n",
      "  0.29753405  0.34142345  0.25616652  0.29530433  0.40258932  0.50459844\n",
      "  0.28542688  0.42672732  0.33276501  0.40756521  0.41742694  0.4032791\n",
      "  0.31908539  0.3914195   0.40798852  0.45063835  0.31320873  0.30122325\n",
      "  0.32699326  0.41873783  0.40616372  0.36191589  0.40447205  0.40087923\n",
      "  0.35363293  0.42589685  0.2878893   0.34081039  0.38687393  0.30863994\n",
      "  0.36518908  0.23057176  0.35498157  0.32528931  0.39180145  0.29188162\n",
      "  0.40403381  0.4208824   0.40018749  0.34119013  0.32065019  0.42231494\n",
      "  0.43316773  0.24125355  0.31383646  0.30544493  0.48911738  0.4054029\n",
      "  0.36057791  0.30808485  0.44429868  0.41513947  0.32056233  0.38124475\n",
      "  0.3835642   0.3780857   0.33101165  0.42894548  0.31263015  0.39345768\n",
      "  0.31942719  0.44831091]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# writer = tf.summary.FileWriter('./graphs/logreg_placeholder', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\t\n",
    "\t# train the model n_epochs times\n",
    "    for i in range(n_epochs): \n",
    "        total_loss = 0\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, entropy], {X: X_batch, Y:Y_batch}) \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\t# test the model\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, {X: X_batch, Y:Y_batch})\n",
    "        total_correct_preds += accuracy_batch\n",
    "        \n",
    "    writer = tf.summary.FileWriter(\"./Downloads/XOR_logs\", sess.graph)\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def parse_data(path, dataset, flatten):\n",
    "    if dataset != 'train' and dataset != 't10k':\n",
    "        raise NameError('dataset must be train or t10k')\n",
    "\n",
    "    label_file = os.path.join(path, dataset + '-labels.idx1-ubyte')\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
    "        new_labels = np.zeros((num, 10))\n",
    "        new_labels[np.arange(num), labels] = 1\n",
    "    \n",
    "    img_file = os.path.join(path, dataset + '-images.idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if flatten:\n",
    "            imgs = imgs.reshape([num, -1])\n",
    "\n",
    "    return imgs, new_labels\n",
    "\n",
    "def read_mnist(path, flatten=True, num_train=55000):\n",
    "    \"\"\"\n",
    "    Read in the mnist dataset, given that the data is stored in path\n",
    "    Return two tuples of numpy arrays\n",
    "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
    "    \"\"\"\n",
    "    imgs, labels = parse_data(path, 'train', flatten)\n",
    "    indices = np.random.permutation(labels.shape[0])\n",
    "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
    "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
    "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
    "    test = parse_data(path, 't10k', flatten)\n",
    "    return (train_img, train_labels), (val_img, val_labels), test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = read_mnist(\"/home/n/data_/mnist/\", flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Starter code for simple logistic regression model for MNIST\n",
    "with tf.data module\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "Created by Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "CS20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Lecture 03\n",
    "\"\"\"\n",
    "\n",
    "# Define paramaters for the model\n",
    "\n",
    "# train, val, test = read_mnist(\"/home/n/data/mnist/\", flatten=True)\n",
    "# Step 2: Create datasets and iterator\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(1000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(500)\n",
    "\n",
    "# create testing Dataset and batch it\n",
    "\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "\n",
    "\n",
    "test_data = test_data.shuffle(1000)\n",
    "test_data = test_data.batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.constant(0)\n",
    "# global_step = tf.get_variable(name=\"exp_decay\",initializer=init)\n",
    "# # starter_learning_rate = 0.1\n",
    "# # learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "# #                                            100000, 0.96, staircase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter_learning_rate = 0.1\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            100000, 0.96, staircase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 500\n",
    "n_epochs = 100\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_init = iterator.make_initializer(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-8895a70a2246>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "\n",
    "# img, label = iterator.get_next()\n",
    "\n",
    "# train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
    "# test_init = iterator.make_initializer(test_data)\t# initializer for train_data\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w1 = tf.get_variable(name=\"weights1\",shape=[784,10],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable(name=\"bias1\",shape=[500,1],dtype=tf.float32,initializer=tf.zeros_initializer())\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(img,w1) + b1\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=label,name=\"loss\")\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Step 6: define optimizer\n",
    "# using Adamn Optimizer with pre-defined learning rate to minimize loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-001).minimize(loss)\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.6643735407428308\n",
      "Average loss epoch 1: 0.3801164736801928\n",
      "Average loss epoch 2: 0.3995897929776799\n",
      "Average loss epoch 3: 0.41392601972276516\n",
      "Average loss epoch 4: 0.4046335613185709\n",
      "Average loss epoch 5: 0.4321628126231107\n",
      "Average loss epoch 6: 0.4293296169150959\n",
      "Average loss epoch 7: 0.40997456447644665\n",
      "Average loss epoch 8: 0.4091814168474891\n",
      "Average loss epoch 9: 0.41277822797948666\n",
      "Total time: 4.976194858551025 seconds\n",
      "Train Accuracy 0.8303\n",
      "Test Accuracy 0.9038\n"
     ]
    }
   ],
   "source": [
    "# writer = tf.summary.FileWriter('./graphs/logreg', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train the model n_epochs times\n",
    "    for i in range(10): \t\n",
    "        sess.run(train_init)\t# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        total_acc= 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, a = sess.run([optimizer, loss,accuracy])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "                total_acc +=a\n",
    "#                 print(\"{}\".format(global_step))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Train Accuracy {0}'.format(total_acc/n_train))\n",
    "\n",
    "    # test the model\n",
    "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('Test Accuracy {0}'.format(total_correct_preds/n_test))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression cannot find best hyperplane for spration of multi category objects, so we choose CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def parse_data(path, dataset, flatten):\n",
    "    if dataset != 'train' and dataset != 't10k':\n",
    "        raise NameError('dataset must be train or t10k')\n",
    "\n",
    "    label_file = os.path.join(path, dataset + '-labels.idx1-ubyte')\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
    "        new_labels = np.zeros((num, 10))\n",
    "        new_labels[np.arange(num), labels] = 1\n",
    "    \n",
    "    img_file = os.path.join(path, dataset + '-images.idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if flatten:\n",
    "            imgs = imgs.reshape([num, -1])\n",
    "\n",
    "    return imgs, new_labels\n",
    "\n",
    "def read_mnist(path, flatten=True, num_train=55000):\n",
    "    \"\"\"\n",
    "    Read in the mnist dataset, given that the data is stored in path\n",
    "    Return two tuples of numpy arrays\n",
    "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
    "    \"\"\"\n",
    "    imgs, labels = parse_data(path, 'train', flatten)\n",
    "    indices = np.random.permutation(labels.shape[0])\n",
    "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
    "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
    "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
    "    test = parse_data(path, 't10k', flatten)\n",
    "    return (train_img, train_labels), (val_img, val_labels), test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = read_mnist(\"/home/n/data_/mnist/\", flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000)\n",
    "train_data = train_data.batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "test_data = test_data.shuffle(10000)\n",
    "test_data = test_data.batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,name):\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    return tf.get_variable(name=name,shape=shape,dtype=tf.float32,initializer=initializer)\n",
    "\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.get_variable(name=name,dtype=tf.float32,initializer=initial)\n",
    "\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    w_1 = weight_variable([5,5,1,32],\"filter_5_32\")\n",
    "    b_1 = bias_variable([32],\"bias_32\")\n",
    "    z_1 = conv2d(x,w_1) + b_1\n",
    "    a_1 = tf.nn.leaky_relu(z_1)\n",
    "    max_1 = max_pool_2x2(a_1)\n",
    "    \n",
    "    w_2 = weight_variable([3,3,32,64],\"filter_3_64\")\n",
    "    b_2 = bias_variable([64],\"bias_64\")\n",
    "    z_2 = conv2d(max_1,w_2) + b_2\n",
    "    a_2 = tf.nn.leaky_relu(z_2)\n",
    "    max_2 = max_pool_2x2(a_2)\n",
    "    \n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024],\"weights_fully_1\")\n",
    "    b_fc1 = bias_variable([1024],\"bias_fc_1\")\n",
    "    h_pool2 = tf.reshape(max_2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.leaky_relu(tf.matmul(h_pool2, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10],\"weights_fc_2\")\n",
    "    b_fc2 = bias_variable([10],\"bias_fc_2\")\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "\n",
    "#     h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#     W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "#     b_conv2 = bias_variable([64])\n",
    "#     h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "#     h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#     W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "#     b_fc1 = bias_variable([1024])\n",
    "#     h_pool2 = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "#     h_fc1 = tf.nn.relu(tf.matmul(h_pool2, W_fc1) + b_fc1)\n",
    "\n",
    "#     W_fc2 = weight_variable([1024, 10])\n",
    "#     b_fc2 = bias_variable([10])\n",
    "#     logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "#     return logits\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cnn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'name:0' shape=(32,) dtype=float32_ref>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial = tf.constant(0.1,shape = [32])\n",
    "tf.get_variable(name=\"name\",dtype=tf.float32,initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=label,name=\"loss\")\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-001).minimize(loss)\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 500\n",
    "n_epochs = 100\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 2854.7394268512726\n",
      "Average loss epoch 1: 19.69341050928289\n",
      "Average loss epoch 2: 7.26179573319175\n",
      "Average loss epoch 3: 4.6437086798928\n",
      "Average loss epoch 4: 3.2164720429615543\n",
      "Average loss epoch 5: 2.6859391900626095\n",
      "Average loss epoch 6: 1.9979158585721797\n",
      "Average loss epoch 7: 6.608248834718357\n",
      "Average loss epoch 8: 1.7743733509020372\n",
      "Average loss epoch 9: 1.322341078248891\n",
      "Average loss epoch 10: 1.21354782784527\n",
      "Average loss epoch 11: 1.029278305647048\n",
      "Average loss epoch 12: 0.8007453016259454\n",
      "Average loss epoch 13: 1.0351594422012567\n",
      "Average loss epoch 14: 0.8446665408124276\n",
      "Average loss epoch 15: 0.6433708988468755\n",
      "Average loss epoch 16: 0.6299202058125626\n",
      "Average loss epoch 17: 0.6672363266348839\n",
      "Average loss epoch 18: 0.5294517819854346\n",
      "Average loss epoch 19: 0.7547389128668742\n",
      "Total time: 135.70124793052673 seconds\n",
      "Train Accuracy 0.9015333333333333\n",
      "Test Accuracy 0.9854\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "   \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train the model n_epochs times\n",
    "    for i in range(20): \t\n",
    "        sess.run(train_init)\t# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        total_acc= 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, a = sess.run([optimizer, loss,accuracy])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "                total_acc +=a\n",
    "#                 print(\"{}\".format(global_step))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Train Accuracy {0}'.format(total_acc/n_train))\n",
    "\n",
    "    # test the model\n",
    "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('Test Accuracy {0}'.format(total_correct_preds/n_test))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def parse_data(path, dataset, flatten):\n",
    "    if dataset != 'train' and dataset != 't10k':\n",
    "        raise NameError('dataset must be train or t10k')\n",
    "\n",
    "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
    "        new_labels = np.zeros((num, 10))\n",
    "        new_labels[np.arange(num), labels] = 1\n",
    "    \n",
    "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if flatten:\n",
    "            imgs = imgs.reshape([num, -1])\n",
    "\n",
    "    return imgs, new_labels\n",
    "\n",
    "def read_mnist(path, flatten=True, num_train=55000):\n",
    "    \"\"\"\n",
    "    Read in the mnist dataset, given that the data is stored in path\n",
    "    Return two tuples of numpy arrays\n",
    "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
    "    \"\"\"\n",
    "    imgs, labels = parse_data(path, 'train', flatten)\n",
    "    indices = np.random.permutation(labels.shape[0])\n",
    "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
    "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
    "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
    "    test = parse_data(path, 't10k', flatten)\n",
    "    return (train_img, train_labels), (val_img, val_labels), test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOllow the instruction here https://github.com/davidflanagan/notMNIST-to-MNIST to ocnvert notMNIST to MNIST data format, everything will same there after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# mnist = NotMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = read_mnist(\"/home/n/notMNIST/\", flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000)\n",
    "train_data = train_data.batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)\n",
    "img,label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "test_data = test_data.shuffle(10000)\n",
    "test_data = test_data.batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,name):\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    return tf.get_variable(name=name,shape=shape,dtype=tf.float32,initializer=initializer)\n",
    "\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.get_variable(name=name,dtype=tf.float32,initializer=initial)\n",
    "\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def conv2dd(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    w_1 = weight_variable([5,5,1,32],\"filter_5_32\")\n",
    "    b_1 = bias_variable([32],\"bias_32\")\n",
    "    z_1 = conv2d(x,w_1) + b_1\n",
    "    z_1 = tf.layers.batch_normalization(z_1)\n",
    "    a_1 = tf.nn.leaky_relu(z_1)\n",
    "    max_1 = max_pool_2x2(a_1)\n",
    "    \n",
    "    \n",
    "    w_2 = weight_variable([3,3,32,64],\"filter_3_64\")\n",
    "    b_2 = bias_variable([64],\"bias_64\")\n",
    "    z_2 = conv2d(max_1,w_2) + b_2\n",
    "    z_2 = tf.layers.batch_normalization(z_2)\n",
    "    a_2 = tf.nn.leaky_relu(z_2)\n",
    "    max_2 = max_pool_2x2(a_2)\n",
    "    \n",
    "    w_3 = weight_variable([3,3,64,128],\"filter_3_128\")\n",
    "    b_3 = bias_variable([128],\"bias_128\")\n",
    "    z_3 = conv2d(max_2,w_3) + b_3\n",
    "    z_3 = tf.layers.batch_normalization(z_3)\n",
    "    a_3 = tf.nn.leaky_relu(z_3)\n",
    "    max_3 = max_pool_2x2(a_3)\n",
    "    \n",
    "    \n",
    "    w_4 = weight_variable([1,1,128,256],\"filter_1_256\")\n",
    "    b_4 = bias_variable([256],\"bias_256\")\n",
    "    z_4 = conv2d(max_3,w_4) + b_4\n",
    "    z_4 = tf.layers.batch_normalization(z_4)\n",
    "    a_4 = tf.nn.leaky_relu(z_4)\n",
    "    max_4 = max_pool_2x2(a_4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    W_fc1 = weight_variable([2 * 2 * 256, 4096],\"weights_fully_1\")\n",
    "    b_fc1 = bias_variable([4096],\"bias_fc_1\")\n",
    "    h_pool2 = tf.reshape(max_4, [-1, 2 * 2 * 256])\n",
    "    h_fc1 = tf.nn.leaky_relu(tf.matmul(h_pool2, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = weight_variable([4096, 10],\"weights_fc_2\")\n",
    "    b_fc2 = bias_variable([10],\"bias_fc_2\")\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    w_1 = weight_variable([5,5,1,32],\"filter_5_32\")\n",
    "    b_1 = bias_variable([32],\"bias_32\")\n",
    "    z_1 = conv2d(x,w_1) + b_1\n",
    "    z_1 = tf.layers.batch_normalization(z_1)\n",
    "\n",
    "    a_1 = tf.nn.leaky_relu(z_1)\n",
    "    max_1 = max_pool_2x2(a_1)\n",
    "    \n",
    "    w_2 = weight_variable([3,3,32,64],\"filter_3_64\")\n",
    "    b_2 = bias_variable([64],\"bias_64\")\n",
    "    z_2 = conv2d(max_1,w_2) + b_2\n",
    "    z_2 = tf.layers.batch_normalization(z_2)\n",
    "\n",
    "    a_2 = tf.nn.leaky_relu(z_2)\n",
    "    max_2 = max_pool_2x2(a_2)\n",
    "    \n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024],\"weights_fully_1\")\n",
    "    b_fc1 = bias_variable([1024],\"bias_fc_1\")\n",
    "    h_pool2 = tf.reshape(max_2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.nn.leaky_relu(tf.matmul(h_pool2, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10],\"weights_fc_2\")\n",
    "    b_fc2 = bias_variable([10],\"bias_fc_2\")\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cnn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 500\n",
    "n_epochs = 1\n",
    "n_train = 100000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-db971087ae75>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=label,name=\"loss\")\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-001).minimize(loss)\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=0.001,decay=0.1).minimize(loss)\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################\n",
    "\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 2494.3664691729978\n",
      "Average loss epoch 1: 29.512358392368665\n",
      "Average loss epoch 2: 9.650451762026007\n",
      "Average loss epoch 3: 6.140543746948242\n",
      "Average loss epoch 4: 3.7728301752697337\n",
      "Average loss epoch 5: 2.491534545204856\n",
      "Average loss epoch 6: 4.208650904352015\n",
      "Average loss epoch 7: 7712569.302576491\n",
      "Average loss epoch 8: 5429204.13022017\n",
      "Average loss epoch 9: 36372.73860085227\n",
      "Average loss epoch 10: 31981.796537642047\n",
      "Average loss epoch 11: 22910.331312144885\n",
      "Average loss epoch 12: 17533.579438920453\n",
      "Average loss epoch 13: 14494.588929332387\n",
      "Total time: 122.05465817451477 seconds\n",
      "Train Accuracy 0.42076\n",
      "Test Accuracy 0.8457\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "   \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train the model n_epochs times\n",
    "    for i in range(14): \t\n",
    "        sess.run(train_init)\t# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        total_acc= 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, a = sess.run([optimizer, loss,accuracy])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "                total_acc +=a\n",
    "#                 print(\"{}\".format(global_step))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Train Accuracy {0}'.format(total_acc/n_train))\n",
    "\n",
    "    # test the model\n",
    "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('Test Accuracy {0}'.format(total_correct_preds/n_test))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    w_1 = weight_variable([5,5,1,32],\"filter_5_32\")\n",
    "    b_1 = bias_variable([32],\"bias_32\")\n",
    "    z_1 = conv2d(x,w_1) + b_1\n",
    "    a_1 = tf.nn.leaky_relu(z_1)\n",
    "    max_1 = max_pool_2x2(a_1)\n",
    "    \n",
    "    w_2 = weight_variable([3,3,32,64],\"filter_3_64\")\n",
    "    b_2 = bias_variable([64],\"bias_64\")\n",
    "    z_2 = conv2d(max_1,w_2) + b_2\n",
    "    a_2 = tf.nn.leaky_relu(z_2)\n",
    "    max_2 = max_pool_2x2(a_2)\n",
    "    \n",
    "#     w_3 = weight_variable([3,3,64,128],\"filter_3_128\")\n",
    "#     b_3 = bias_variable([128],\"bias_128\")\n",
    "#     z_3 = conv2d_(max_2,w_3) + b_3\n",
    "#     a_3 = tf.nn.leaky_relu(z_3)\n",
    "#     max_3 = max_pool_2x2(a_3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    W_fc1 = weight_variable([2 * 2 * 128, 1024],\"weights_fully_1\")\n",
    "    b_fc1 = bias_variable([1024],\"bias_fc_1\")\n",
    "    h_pool2 = tf.reshape(max_2, [-1, 2 * 2 * 128])\n",
    "    h_fc1 = tf.nn.leaky_relu(tf.matmul(h_pool2, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10],\"weights_fc_2\")\n",
    "    b_fc2 = bias_variable([10],\"bias_fc_2\")\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    return logits\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

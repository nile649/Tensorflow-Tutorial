{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from six.moves import urllib\n",
    "\n",
    "def get_resized_image(img_path, width, height, save=True):\n",
    "    image = Image.open(img_path)\n",
    "    # PIL is column major so you have to swap the places of width and height\n",
    "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
    "    if save:\n",
    "        image_dirs = img_path.split('/')\n",
    "        image_dirs[-1] = 'resized_' + image_dirs[-1]\n",
    "        out_path = '/'.join(image_dirs)\n",
    "        if not os.path.exists(out_path):\n",
    "            image.save(out_path)\n",
    "    image = np.asarray(image, np.float32)\n",
    "    return np.expand_dims(image, 0)\n",
    "\n",
    "def generate_noise_image(content_image, width, height, noise_ratio=0.6):\n",
    "    noise_image = np.random.uniform(-20, 20, (1, height, width, 3)).astype(np.float32)\n",
    "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "def save_image(path, image):\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# VGG-19 parameters file\n",
    "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "VGG_FILENAME = 'imagenet-vgg-verydeep-19.mat'\n",
    "EXPECTED_BYTES = 534904783\n",
    "\n",
    "class VGG(object):\n",
    "    def __init__(self, input_img):\n",
    "#         utils.download(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
    "        self.vgg_layers = scipy.io.loadmat('/home/n/data/imagenet-vgg-verydeep-19.mat')['layers']\n",
    "        self.input_img = input_img\n",
    "        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\n",
    "    def _weights(self, layer_idx, expected_layer_name):\n",
    "        \"\"\" Return the weights and biases at layer_idx already trained by VGG\n",
    "        \"\"\"\n",
    "        W = self.vgg_layers[0][layer_idx][0][0][2][0][0]\n",
    "        b = self.vgg_layers[0][layer_idx][0][0][2][0][1]\n",
    "        layer_name = self.vgg_layers[0][layer_idx][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b.reshape(b.size)\n",
    "\n",
    "    def conv2d_relu(self, prev_layer, layer_idx, layer_name):\n",
    "        \"\"\" Return the Conv2D layer with RELU using the weights, \n",
    "        biases from the VGG model at 'layer_idx'.\n",
    "        Don't forget to apply relu to the output from the convolution.\n",
    "        Inputs:\n",
    "            prev_layer: the output tensor from the previous layer\n",
    "            layer_idx: the index to current layer in vgg_layers\n",
    "            layer_name: the string that is the name of the current layer.\n",
    "                        It's used to specify variable_scope.\n",
    "\n",
    "\n",
    "        Note that you first need to obtain W and b from from the corresponding VGG's layer \n",
    "        using the function _weights() defined above.\n",
    "        W and b returned from _weights() are numpy arrays, so you have\n",
    "        to convert them to TF tensors. One way to do it is with tf.constant.\n",
    "\n",
    "        Hint for choosing strides size: \n",
    "            for small images, you probably don't want to skip any pixel\n",
    "        \"\"\"\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        with tf.variable_scope(layer_name) as scope:\n",
    "            W, b = self._weights(layer_idx, layer_name)\n",
    "            W = tf.constant(W, name='weights')\n",
    "            b = tf.constant(b, name='bias')\n",
    "            conv2d = tf.nn.conv2d(prev_layer, \n",
    "                                filter=W, \n",
    "                                strides=[1, 1, 1, 1], \n",
    "                                padding='SAME')\n",
    "            out = tf.nn.relu(conv2d + b)\n",
    "        ###############################\n",
    "        setattr(self, layer_name, out)\n",
    "\n",
    "    def avgpool(self, prev_layer, layer_name):\n",
    "        \"\"\" Return the average pooling layer. The paper suggests that \n",
    "        average pooling works better than max pooling.\n",
    "        Input:\n",
    "            prev_layer: the output tensor from the previous layer\n",
    "            layer_name: the string that you want to name the layer.\n",
    "                        It's used to specify variable_scope.\n",
    "\n",
    "        Hint for choosing strides and kszie: choose what you feel appropriate\n",
    "        \"\"\"\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        with tf.variable_scope(layer_name):\n",
    "            out = tf.nn.avg_pool(prev_layer, \n",
    "                                ksize=[1, 2, 2, 1], \n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "        ###############################\n",
    "        setattr(self, layer_name, out)\n",
    "\n",
    "    def load(self):\n",
    "        self.conv2d_relu(self.input_img, 0, 'conv1_1')\n",
    "        self.conv2d_relu(self.conv1_1, 2, 'conv1_2')\n",
    "        self.avgpool(self.conv1_2, 'avgpool1')\n",
    "        self.conv2d_relu(self.avgpool1, 5, 'conv2_1')\n",
    "        self.conv2d_relu(self.conv2_1, 7, 'conv2_2')\n",
    "        self.avgpool(self.conv2_2, 'avgpool2')\n",
    "        self.conv2d_relu(self.avgpool2, 10, 'conv3_1')\n",
    "        self.conv2d_relu(self.conv3_1, 12, 'conv3_2')\n",
    "        self.conv2d_relu(self.conv3_2, 14, 'conv3_3')\n",
    "        self.conv2d_relu(self.conv3_3, 16, 'conv3_4')\n",
    "        self.avgpool(self.conv3_4, 'avgpool3')\n",
    "        self.conv2d_relu(self.avgpool3, 19, 'conv4_1')\n",
    "        self.conv2d_relu(self.conv4_1, 21, 'conv4_2')\n",
    "        self.conv2d_relu(self.conv4_2, 23, 'conv4_3')\n",
    "        self.conv2d_relu(self.conv4_3, 25, 'conv4_4')\n",
    "        self.avgpool(self.conv4_4, 'avgpool4')\n",
    "        self.conv2d_relu(self.avgpool4, 28, 'conv5_1')\n",
    "        self.conv2d_relu(self.conv5_1, 30, 'conv5_2')\n",
    "        self.conv2d_relu(self.conv5_2, 32, 'conv5_3')\n",
    "        self.conv2d_relu(self.conv5_3, 34, 'conv5_4')\n",
    "        self.avgpool(self.conv5_4, 'avgpool5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name content loss is illegal; using content_loss instead.\n",
      "INFO:tensorflow:Summary name style loss is illegal; using style_loss instead.\n",
      "INFO:tensorflow:Summary name total loss is illegal; using total_loss instead.\n",
      "8801412000.0\n",
      "Step 1\n",
      "   Sum: 127200600.9\n",
      "   Loss: 8801412096.0\n",
      "   Took: 9.416696310043335 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8107839000.0\n",
      "Step 2\n",
      "   Sum: 127261799.0\n",
      "   Loss: 8107838976.0\n",
      "   Took: 0.7152948379516602 seconds\n",
      "7444877000.0\n",
      "Step 3\n",
      "   Sum: 127331334.9\n",
      "   Loss: 7444876800.0\n",
      "   Took: 0.6583847999572754 seconds\n",
      "6804821000.0\n",
      "Step 4\n",
      "   Sum: 127406804.9\n",
      "   Loss: 6804820992.0\n",
      "   Took: 0.6678211688995361 seconds\n",
      "6187760000.0\n",
      "Step 5\n",
      "   Sum: 127486138.6\n",
      "   Loss: 6187760128.0\n",
      "   Took: 0.6688907146453857 seconds\n",
      "3742235400.0\n",
      "Step 10\n",
      "   Sum: 127901594.2\n",
      "   Loss: 3742235392.0\n",
      "   Took: 1.894413948059082 seconds\n",
      "2070123000.0\n",
      "Step 20\n",
      "   Sum: 128534318.3\n",
      "   Loss: 2070123008.0\n",
      "   Took: 3.451385259628296 seconds\n",
      "923767700.0\n",
      "Step 40\n",
      "   Sum: 128770931.1\n",
      "   Loss: 923767680.0\n",
      "   Took: 6.6175031661987305 seconds\n",
      "503515500.0\n",
      "Step 60\n",
      "   Sum: 128466390.3\n",
      "   Loss: 503515488.0\n",
      "   Took: 6.4967663288116455 seconds\n",
      "328774460.0\n",
      "Step 80\n",
      "   Sum: 127980380.1\n",
      "   Loss: 328774464.0\n",
      "   Took: 6.494952440261841 seconds\n",
      "239084320.0\n",
      "Step 100\n",
      "   Sum: 127524734.4\n",
      "   Loss: 239084320.0\n",
      "   Took: 6.634424924850464 seconds\n",
      "185293490.0\n",
      "Step 120\n",
      "   Sum: 127113802.6\n",
      "   Loss: 185293488.0\n",
      "   Took: 6.562460899353027 seconds\n",
      "149629140.0\n",
      "Step 140\n",
      "   Sum: 126733828.7\n",
      "   Loss: 149629136.0\n",
      "   Took: 6.523815870285034 seconds\n",
      "124231640.0\n",
      "Step 160\n",
      "   Sum: 126372456.7\n",
      "   Loss: 124231640.0\n",
      "   Took: 6.7183849811553955 seconds\n",
      "105302420.0\n",
      "Step 180\n",
      "   Sum: 126024183.7\n",
      "   Loss: 105302416.0\n",
      "   Took: 6.6457133293151855 seconds\n",
      "90926420.0\n",
      "Step 200\n",
      "   Sum: 125686547.4\n",
      "   Loss: 90926416.0\n",
      "   Took: 6.587137460708618 seconds\n",
      "79608310.0\n",
      "Step 220\n",
      "   Sum: 125358005.7\n",
      "   Loss: 79608312.0\n",
      "   Took: 6.706693649291992 seconds\n",
      "70429420.0\n",
      "Step 240\n",
      "   Sum: 125039524.5\n",
      "   Loss: 70429424.0\n",
      "   Took: 6.596389055252075 seconds\n",
      "62828068.0\n",
      "Step 260\n",
      "   Sum: 124731124.0\n",
      "   Loss: 62828068.0\n",
      "   Took: 6.536278963088989 seconds\n",
      "56410120.0\n",
      "Step 280\n",
      "   Sum: 124430743.7\n",
      "   Loss: 56410120.0\n",
      "   Took: 6.56198525428772 seconds\n",
      "50965144.0\n",
      "Step 300\n",
      "   Sum: 124137020.3\n",
      "   Loss: 50965144.0\n",
      "   Took: 6.526128053665161 seconds\n",
      "46280628.0\n",
      "Step 320\n",
      "   Sum: 123850513.0\n",
      "   Loss: 46280628.0\n",
      "   Took: 6.517507791519165 seconds\n",
      "42318270.0\n",
      "Step 340\n",
      "   Sum: 123570896.1\n",
      "   Loss: 42318272.0\n",
      "   Took: 6.524831771850586 seconds\n",
      "38938850.0\n",
      "Step 360\n",
      "   Sum: 123298595.1\n",
      "   Loss: 38938848.0\n",
      "   Took: 6.537346124649048 seconds\n",
      "36020732.0\n",
      "Step 380\n",
      "   Sum: 123033311.9\n",
      "   Loss: 36020732.0\n",
      "   Took: 6.518669128417969 seconds\n",
      "33511164.0\n",
      "Step 400\n",
      "   Sum: 122774834.4\n",
      "   Loss: 33511164.0\n",
      "   Took: 6.660991907119751 seconds\n",
      "31309062.0\n",
      "Step 420\n",
      "   Sum: 122522398.7\n",
      "   Loss: 31309062.0\n",
      "   Took: 6.693472862243652 seconds\n",
      "29352748.0\n",
      "Step 440\n",
      "   Sum: 122275872.5\n",
      "   Loss: 29352748.0\n",
      "   Took: 7.164753437042236 seconds\n",
      "27604128.0\n",
      "Step 460\n",
      "   Sum: 122034855.3\n",
      "   Loss: 27604128.0\n",
      "   Took: 7.059122562408447 seconds\n",
      "26032398.0\n",
      "Step 480\n",
      "   Sum: 121798007.2\n",
      "   Loss: 26032398.0\n",
      "   Took: 7.166724443435669 seconds\n",
      "24592820.0\n",
      "Step 500\n",
      "   Sum: 121566109.0\n",
      "   Loss: 24592820.0\n",
      "   Took: 7.285273790359497 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class StyleTransfer(object):\n",
    "    def __init__(self, content_img, style_img, img_width, img_height):\n",
    "       \n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.content_img = get_resized_image(content_img, img_width, img_height)\n",
    "        self.style_img = get_resized_image(style_img, img_width, img_height)\n",
    "        self.initial_img = generate_noise_image(self.content_img, img_width, img_height)\n",
    "\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        ## create global step (gstep) and hyperparameters for the model\n",
    "        self.content_layer = 'conv4_2'\n",
    "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "        self.content_w = 0.01\n",
    "        self.style_w = 1\n",
    "        self.style_layer_w = [0.5, 1.0, 1.5, 3.0, 4.0] \n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.lr = 2.0\n",
    "        ###############################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def create_input(self):\n",
    "        '''\n",
    "        We will use one input_img as a placeholder for the content image, \n",
    "        style image, and generated image, because:\n",
    "            1. they have the same dimension\n",
    "            2. we have to extract the same set of features from them\n",
    "        We use a variable instead of a placeholder because we're, at the same time, \n",
    "        training the generated image to get the desirable result.\n",
    "\n",
    "        Note: image height corresponds to number of rows, not columns.\n",
    "        '''\n",
    "        with tf.variable_scope('input') as scope:\n",
    "            self.input_img = tf.get_variable('in_img', \n",
    "                                        shape=([1, self.img_height, self.img_width, 3]),\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.zeros_initializer())\n",
    "            \n",
    "    def load_vgg(self):\n",
    "        '''\n",
    "        Load the saved model parameters of VGG-19, using the input_img\n",
    "        as the input to compute the output at each layer of vgg.\n",
    "\n",
    "        During training, VGG-19 mean-centered all images and found the mean pixels\n",
    "        to be [123.68, 116.779, 103.939] along RGB dimensions. We have to subtract\n",
    "        this mean from our images.\n",
    "\n",
    "\n",
    "        On passing any image from content, style, initial image it will create the model for that image\n",
    "        '''\n",
    "        self.vgg = VGG(self.input_img)\n",
    "        self.vgg.load()\n",
    "        self.content_img -= self.vgg.mean_pixels\n",
    "        self.style_img -= self.vgg.mean_pixels\n",
    "        \n",
    "        \n",
    "    def _content_loss(self, P):\n",
    "        ''' Calculate the loss between the feature representation of the\n",
    "        content image and the generated image.\n",
    "        \n",
    "        Inputs: \n",
    "            P: content representation of the content image\n",
    "            F: content representation of the generated image\n",
    "            Read the assignment handout for more details\n",
    "\n",
    "            Note: Don't use the coefficient 0.5 as defined in the paper.\n",
    "            Use the coefficient defined in the assignment handout.\n",
    "        '''\n",
    "        # self.content_loss = None\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        F = getattr(self.vgg,self.content_layer)\n",
    "        self.content_loss = tf.reduce_sum((F - P) ** 2) / (4.0)\n",
    "\n",
    "        \n",
    "    def _gram_matrix(self, F, N, M):\n",
    "        \"\"\" Create and return the gram matrix for tensor F\n",
    "            Hint: you'll first have to reshape F\n",
    "        \"\"\"\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        F = tf.reshape(F, (M, N))\n",
    "        return tf.matmul(tf.transpose(F), F)\n",
    "        ###############################\n",
    "\n",
    "        \n",
    "    def _single_style_loss(self, a, g):\n",
    "        \"\"\" Calculate the style loss at a certain layer\n",
    "        Inputs:\n",
    "            a is the feature representation of the style image at that layer\n",
    "            g is the feature representation of the generated image at that layer\n",
    "        Output:\n",
    "            the style loss at a certain layer (which is E_l in the paper)\n",
    "\n",
    "        Hint: 1. you'll have to use the function _gram_matrix()\n",
    "            2. we'll use the same coefficient for style loss as in the paper\n",
    "            3. a and g are feature representation, not gram matrices\n",
    "        \"\"\"\n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        N = a.shape[3] # number of filters\n",
    "        M = a.shape[1] * a.shape[2] # height times width of the feature map\n",
    "        A = self._gram_matrix(a, N, M)\n",
    "        G = self._gram_matrix(g, N, M)\n",
    "        return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))\n",
    "\n",
    "    \n",
    "    def _style_loss(self, A):\n",
    "        \"\"\" Calculate the total style loss as a weighted sum \n",
    "        of style losses at all style layers\n",
    "        Hint: you'll have to use _single_style_loss()\n",
    "        \"\"\"\n",
    "        n_layers = len(A)\n",
    "        E = [self._single_style_loss(A[i], getattr(self.vgg, self.style_layers[i])) for i in range(n_layers)]\n",
    "        \n",
    "        ###############################\n",
    "        ## TO DO\n",
    "        self.style_loss = sum([self.style_layer_w[i] * E[i] for i in range(n_layers)])\n",
    "\n",
    "    def losses(self):\n",
    "        with tf.variable_scope('losses') as scope:\n",
    "            with tf.Session() as sess:\n",
    "                # assign content image to the input variable\n",
    "                sess.run(self.input_img.assign(self.content_img)) \n",
    "                gen_img_content = getattr(self.vgg, self.content_layer)\n",
    "#                 content_img_content = sess.run(gen_img_content)\n",
    "            self._content_loss(gen_img_content)\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(self.input_img.assign(self.style_img))\n",
    "                style_layers = sess.run([getattr(self.vgg, layer) for layer in self.style_layers])                              \n",
    "            self._style_loss(style_layers)\n",
    "\n",
    "            ##########################################\n",
    "            ## TO DO: create total loss. \n",
    "            ## Hint: don't forget the weights for the content loss and style loss\n",
    "            self.total_loss = self.content_w * self.content_loss + self.style_w * self.style_loss\n",
    "            ##########################################\n",
    "    def optimize(self):\n",
    "        ###############################\n",
    "        ## TO DO: create optimizer\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.total_loss,\n",
    "                                                            global_step=self.gstep)\n",
    "        ###############################\n",
    "\n",
    "    def create_summary(self):\n",
    "        ###############################\n",
    "        ## TO DO: create summaries for all the losses\n",
    "        ## Hint: don't forget to merge them\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('content loss', self.content_loss)\n",
    "            tf.summary.scalar('style loss', self.style_loss)\n",
    "            tf.summary.scalar('total loss', self.total_loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        ###############################\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        self.create_input()\n",
    "        self.load_vgg()\n",
    "        self.losses()\n",
    "        self.optimize()\n",
    "        self.create_summary()\n",
    "\n",
    "    def train(self, n_iters):\n",
    "        skip_step = 1\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            ###############################\n",
    "            ## TO DO: \n",
    "            ## 1. initialize your variables\n",
    "            ## 2. create writer to write your graph\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            writer = tf.summary.FileWriter('graphs/style_stranfer', sess.graph)\n",
    "            ###############################\n",
    "            sess.run(self.input_img.assign(self.initial_img))\n",
    "\n",
    "\n",
    "            ###############################\n",
    "            ## TO DO: \n",
    "            ## 1. create a saver object\n",
    "            ## 2. check if a checkpoint exists, restore the variables\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/style_transfer/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            ##############################\n",
    "\n",
    "            initial_step = self.gstep.eval()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for index in range(initial_step, n_iters):\n",
    "                if index >= 5 and index < 20:\n",
    "                    skip_step = 10\n",
    "                elif index >= 20:\n",
    "                    skip_step = 20\n",
    "                \n",
    "                sess.run(self.opt)\n",
    "                if (index + 1) % skip_step == 0:\n",
    "                    ###############################\n",
    "                    ## TO DO: obtain generated image, loss, and summary\n",
    "                    gen_image, total_loss, summary = sess.run([self.input_img,\n",
    "                                                                self.total_loss,\n",
    "                                                                self.summary_op])\n",
    "                    print(self.style_loss.eval(session=sess))\n",
    "\n",
    "\n",
    "                    ###############################\n",
    "                    \n",
    "                    # add back the mean pixels we subtracted before\n",
    "                    gen_image = gen_image + self.vgg.mean_pixels \n",
    "                    writer.add_summary(summary, global_step=index)\n",
    "                    print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "                    print('   Loss: {:5.1f}'.format(total_loss))\n",
    "                    print('   Took: {} seconds'.format(time.time() - start_time))\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    filename = 'outputs/%d.png' % (index)\n",
    "                    save_image(filename, gen_image)\n",
    "\n",
    "                    if (index + 1) % 20 == 0:\n",
    "                        ###############################\n",
    "                        ## TO DO: save the variables into a checkpoint\n",
    "                        saver.save(sess, 'checkpoints/style_stranfer/style_transfer', index)\n",
    "                        ###############################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     setup()\n",
    "    machine = StyleTransfer('/home/n/neural-style/images/makise.jpg', '/home/n/smooth_ride.jpg',500,500)\n",
    "    machine.build()\n",
    "    machine.train(500)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
